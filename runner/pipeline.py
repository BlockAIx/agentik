"""pipeline.py â€” Task pipeline orchestration and main entry point."""

import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

from runner.config import (
    MAX_ATTEMPTS,
    MAX_PARALLEL_AGENTS,
    ROADMAP_FILENAME,
    _console,
    set_verbose,
)
from runner.opencode import (
    check_models,
    check_monthly_budget,
    run_opencode_build,
    run_opencode_document,
    run_opencode_milestone,
    run_opencode_static_fix,
    select_project,
)
from runner.roadmap import (
    get_ready_tasks,
    get_task_body,
    get_task_ecosystem,
    get_task_outputs,
    get_task_version,
    get_tasks,
    is_milestone_task,
    parse_task_graph,
    run_static_checks,
    run_tests,
)
from runner.state import (
    _format_tokens,
    load_project_budget,
    load_runner_state,
    mark_done,
    save_runner_state,
    task_done,
)
from runner.workspace import (
    commit_and_merge,
    ensure_workspace_dirs,
    generate_project_agents_md,
    install_project_dependencies,
    scaffold_ecosystem_configs,
    tag_milestone,
    try_deploy_hook,
)

# â”€â”€ Attempt pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def run_attempt(
    task: str, attempt: int, fix_logs: str | None, project_dir: Path
) -> tuple[bool, str | None]:
    """Run one build â†’ test cycle for *task*.

    Args:
        task:        ROADMAP heading.
        attempt:     Zero-based attempt index.
        fix_logs:    Previous test output, or None on first attempt.
        project_dir: Project directory path.

    Returns:
        ``(True, None)`` on success; ``(False, test_output)`` on failure.
    """
    phase_label = "Build" if fix_logs is None else "Fix"
    _console.print(
        f"\n[bold][1/5] {phase_label}[/]  [dim](attempt {attempt + 1}/{MAX_ATTEMPTS})[/]"
    )

    # On the first attempt, scaffold any ecosystem config files this task needs.
    if fix_logs is None:
        task_eco = get_task_ecosystem(task, project_dir)
        scaffold_ecosystem_configs(project_dir, task_eco)

    run_opencode_build(task, project_dir, fix_logs=fix_logs, attempt=attempt)

    _console.print("[dim][deps] Syncing project dependencies...[/]")
    install_project_dependencies(project_dir)

    # Re-run scaffold after build â€” the agent may have created config files
    # (e.g. vite.config.ts) that need patching (host/port binding, tsconfig
    # test types).  scaffold_ecosystem_configs is idempotent.
    task_eco = get_task_ecosystem(task, project_dir)
    scaffold_ecosystem_configs(project_dir, task_eco)

    _console.print("\n[bold][2/6] Test[/]")
    passed, output = run_tests(project_dir)

    # Coverage gating: if tests pass, check coverage threshold.
    if passed:
        from runner.coverage import (  # noqa: PLC0415
            check_coverage_gate,
            get_min_coverage,
            run_tests_with_coverage,
        )

        threshold = get_min_coverage(project_dir)
        if threshold is not None:
            _console.print("[dim]  Checking test coverage...[/]")
            _, cov_output, coverage_pct = run_tests_with_coverage(project_dir)
            cov_ok, cov_msg = check_coverage_gate(project_dir, coverage_pct)
            if coverage_pct is not None:
                _console.print(
                    f"  [dim]Coverage: {coverage_pct:.0f}% (threshold: {threshold}%)[/]"
                )
            if not cov_ok:
                _console.print(f"  [yellow]{cov_msg}[/]")
                return False, f"Coverage below threshold.\n{cov_msg}\n{cov_output}"

    return passed, (None if passed else output)


_STATIC_FIX_MAX_ATTEMPTS = 2


def _handle_task_failure(
    task: str, project_dir: Path, fix_logs: str | None = None
) -> None:
    """Handle a task that has exhausted all retry attempts.

    1. Save a structured failure report (diagnostics).
    2. Rollback the git feature branch.
    3. Mark the task as failed in runner state.
    4. Send a webhook notification if configured.
    """
    from runner.diagnostics import save_failure_report  # noqa: PLC0415
    from runner.rollback import (  # noqa: PLC0415
        mark_task_failed,
        rollback_feature_branch,
    )

    # Collect tokens spent on this task from budget.
    budget = load_project_budget(project_dir)
    tokens_spent = sum(
        s.get("tokens", 0) for s in budget.get("sessions", []) if s.get("task") == task
    )

    # Save failure report.
    save_failure_report(
        task,
        project_dir,
        attempts=MAX_ATTEMPTS,
        tokens_spent=tokens_spent,
        fix_logs=fix_logs,
    )

    # Rollback git changes.
    rollback_feature_branch(task, project_dir)

    # Mark as failed (not done).
    mark_task_failed(
        task,
        project_dir,
        {
            "attempts": MAX_ATTEMPTS,
            "tokens_spent": tokens_spent,
        },
    )

    # Send notification.
    try:
        from runner.notify import send_notification  # noqa: PLC0415

        send_notification(
            project_dir,
            "task_failed",
            task=task,
            status="failed",
            details={"attempts": MAX_ATTEMPTS, "tokens_spent": tokens_spent},
        )
    except Exception:  # noqa: BLE001
        pass

    _console.print(f"\n[red bold]âœ— Failed:[/] {task} (after {MAX_ATTEMPTS} attempts)")


def finalise_task(
    task: str, project_dir: Path, task_outputs: list[str] | None = None
) -> None:
    """Static-check, document, review (optional), commit, merge, and tag a successfully tested task."""
    _console.print("\n[bold][3/6] Static checks[/]")
    for _attempt in range(_STATIC_FIX_MAX_ATTEMPTS):
        ok, check_output = run_static_checks(project_dir)
        if ok:
            _console.print("[green]Static checks passed.[/]")
            break
        _console.print(
            f"[yellow]Static analysis issues found (attempt {_attempt + 1}/{_STATIC_FIX_MAX_ATTEMPTS}).[/]"
        )
        run_opencode_static_fix(task, project_dir, check_output)
    else:
        _console.print(
            "[yellow]âš  Static analysis still failing after max attempts â€” proceeding.[/]"
        )

    _console.print("\n[bold][4/6] Document[/]")
    run_opencode_document(task, project_dir)

    # â”€â”€ Human-in-the-loop review (opt-in) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from runner.review import (  # noqa: PLC0415
        discard_changes,
        is_review_enabled,
        show_diff_and_ask,
    )

    if is_review_enabled(task, project_dir):
        _console.print("\n[bold][5/6] Human Review[/]")
        decision = show_diff_and_ask(project_dir)
        if decision == "reject":
            discard_changes(project_dir)
            _console.print("[yellow]Task rejected by reviewer â€” changes discarded.[/]")
            return
        _console.print("[green]Review approved.[/]")
    else:
        _console.print("\n[dim][5/6] Review â€” skipped (not enabled)[/]")

    _console.print("\n[bold][6/6] Commit & merge[/]")
    mark_done(task, project_dir)
    commit_and_merge(task, project_dir, task_outputs=task_outputs)
    try_deploy_hook(task, project_dir)

    # Send success notification.
    try:
        from runner.notify import send_notification  # noqa: PLC0415

        send_notification(project_dir, "task_complete", task=task, status="ok")
    except Exception:  # noqa: BLE001
        pass

    _console.print(f"\n[green bold]âœ” Completed:[/] {task}")


def _validate_roadmap(project_dir: Path) -> None:
    """Run ``helpers/check_roadmap`` and abort (exit 1) on structural errors."""
    from helpers.check_roadmap import run_checks  # noqa: PLC0415

    _console.print()
    _console.rule("[bold]ROADMAP Validation[/]", style="dim")
    rc = run_checks(project_dir / ROADMAP_FILENAME)
    if rc != 0:
        _console.print(
            "\n[red bold]ROADMAP has structural errors. "
            "Fix them before running the pipeline.[/]"
        )
        sys.exit(1)
    _console.print()


def process_task(
    task: str,
    project_dir: Path,
    resume_attempt: int = 0,
    resume_fix_logs: str | None = None,
) -> None:
    """Drive a single ROADMAP task through build â†’ test â†’ fix â†’ finalise.

    Args:
        task:              ROADMAP heading.
        project_dir:       Project directory path.
        resume_attempt:    Attempt to start from (non-zero on resume).
        resume_fix_logs:   Saved fix logs from last failure, or None.
    """
    _console.print()
    resume_note = (
        f"[dim] â€” resuming from attempt {resume_attempt + 1}[/]"
        if resume_attempt > 0
        else ""
    )
    _console.rule(f"[bold cyan]{task}[/]{resume_note}", style="cyan")

    check_monthly_budget(project_dir=project_dir)

    from runner.workspace import ensure_feature_branch  # noqa: PLC0415

    ensure_feature_branch(task, project_dir)

    fix_logs: str | None = resume_fix_logs

    for attempt in range(resume_attempt, MAX_ATTEMPTS):
        save_runner_state(project_dir, task, attempt, fix_logs)

        passed, fix_logs = run_attempt(task, attempt, fix_logs, project_dir)

        if passed:
            finalise_task(task, project_dir)
            return

        _console.print(f"  [yellow]Tests failed on attempt {attempt + 1}.[/]")
        if attempt == MAX_ATTEMPTS - 1:
            _console.print("[red]Max attempts reached.[/]")
            _handle_task_failure(task, project_dir, fix_logs)
            return


# â”€â”€ Milestone pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def process_milestone(task: str, project_dir: Path) -> None:
    """Process an ``agent: milestone`` task â€” review, merge to main, and tag.

    Milestone tasks are barrier points: they never run in parallel.  The
    milestone agent reviews the project state (read-only), then the runner
    merges develop into main and creates a semver tag.
    """
    _console.print()
    _console.rule(f"[bold magenta]{task}[/]  [dim](milestone)[/]", style="magenta")

    version = get_task_version(task, project_dir)
    if not version:
        # Derive a fallback version from the task number.
        import re  # noqa: PLC0415

        m = re.match(r"^## (\d{3}) - ", task)
        version = f"0.0.{int(m.group(1))}" if m else "0.0.0"

    check_monthly_budget(project_dir=project_dir)
    _console.print(f"\n[bold][1/2] Milestone review[/]  [dim](v{version})[/]")
    run_opencode_milestone(task, version, project_dir)

    _console.print(f"\n[bold][2/2] Tag & merge[/]")
    tag_milestone(version, project_dir)
    mark_done(task, project_dir)
    _console.print(f"\n[green bold]âœ” Milestone:[/] {task} â†’ v{version}")


# â”€â”€ Parallel batch pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def process_parallel_batch(batch: list[str], project_dir: Path) -> None:
    """Build *batch* tasks in parallel, then test/static/document once and commit per-task."""
    from runner.workspace import ensure_feature_branch  # noqa: PLC0415

    _console.print()
    _console.rule("[bold magenta]Parallel Build Batch[/]", style="magenta")
    _console.print(f"[dim]Building {len(batch)} independent tasks in parallel[/]")
    for t in batch:
        _console.print(f"  [dim]â†’ {t}[/]")

    check_monthly_budget(project_dir=project_dir)

    # Pre-scaffold ecosystem configs for all tasks in the batch.
    for task in batch:
        eco = get_task_ecosystem(task, project_dir)
        scaffold_ecosystem_configs(project_dir, eco)

    # â”€â”€ 1/4 Build (parallel) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _console.print(f"\n[bold][1/4] Build[/]  [dim](parallel, {len(batch)} agents)[/]")

    def _build_one(task: str) -> str:
        run_opencode_build(
            task,
            project_dir,
            fix_logs=None,
            attempt=0,
            capture=True,
            parallel_batch=batch,
        )
        return task

    spinner_label = f"[dim]{'build':<12}[/] {len(batch)} tasks in parallel"
    with _console.status(spinner_label, spinner="dots", spinner_style="cyan"):
        with ThreadPoolExecutor(max_workers=len(batch)) as executor:
            futures = {executor.submit(_build_one, t): t for t in batch}
            results: list[tuple[str, Exception | None]] = []
            for future in as_completed(futures):
                task = futures[future]
                try:
                    future.result()
                    results.append((task, None))
                except Exception as exc:
                    results.append((task, exc))
    for task, exc in results:
        if exc is None:
            _console.print(f"  [green]âœ“ Built:[/] {task}")
        else:
            _console.print(f"  [red]âœ— Build failed:[/] {task}: {exc}")
    for task, exc in results:
        if exc is not None:
            raise exc
    _console.print(f"[dim]  All agent output in: {project_dir.name}/logs/[/]")

    # Sync dependencies once after all builds.
    _console.print("[dim][deps] Syncing project dependencies...[/]")
    install_project_dependencies(project_dir)

    # â”€â”€ 2/4 Test (once for the whole batch) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _console.print("\n[bold][2/4] Test[/]  [dim](batch)[/]")
    passed, output = run_tests(project_dir)

    if not passed:
        # Fix loop â€” use first task as context; the agent sees all code + test output.
        fix_logs: str | None = output
        for attempt in range(1, MAX_ATTEMPTS):
            _console.print(
                f"\n[bold][2/4] Fix[/]  [dim](attempt {attempt + 1}/{MAX_ATTEMPTS})[/]"
            )
            save_runner_state(project_dir, batch[0], attempt, fix_logs)
            run_opencode_build(
                batch[0],
                project_dir,
                fix_logs=fix_logs,
                attempt=attempt,
                force_new_session=True,
            )
            install_project_dependencies(project_dir)

            _console.print("\n[bold][2/4] Re-test[/]")
            passed, output = run_tests(project_dir)
            if passed:
                break
            fix_logs = output
        else:
            _console.print("[red]Max fix attempts reached for batch.[/]")
            for t in batch:
                _handle_task_failure(t, project_dir, fix_logs)
            return

    # â”€â”€ 3/4 Static checks (once for the whole batch) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _console.print("\n[bold][3/4] Static checks[/]  [dim](batch)[/]")
    for _attempt in range(_STATIC_FIX_MAX_ATTEMPTS):
        ok, check_output = run_static_checks(project_dir)
        if ok:
            _console.print("[green]Static checks passed.[/]")
            break
        _console.print(
            f"[yellow]Static analysis issues found (attempt {_attempt + 1}/{_STATIC_FIX_MAX_ATTEMPTS}).[/]"
        )
        run_opencode_static_fix(batch[0], project_dir, check_output)
    else:
        _console.print(
            "[yellow]âš  Static analysis still failing after max attempts â€” proceeding.[/]"
        )

    # â”€â”€ 4/4 Document (once) + Commit (per-task for git attribution) â”€â”€â”€â”€â”€â”€
    _console.print("\n[bold][4/4] Document & commit[/]  [dim](batch)[/]")
    run_opencode_document(batch[0], project_dir)

    for idx, task in enumerate(batch):
        is_last = idx == len(batch) - 1
        ensure_feature_branch(task, project_dir)

        outputs = get_task_outputs(task, project_dir)
        # Last task uses `git add .` to catch any files not in any outputs list.
        selective = outputs if outputs and not is_last else None

        mark_done(task, project_dir)
        commit_and_merge(task, project_dir, task_outputs=selective)
        try_deploy_hook(task, project_dir)
        _console.print(f"  [green]âœ” Committed:[/] {task}")


# â”€â”€ Entry point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def main() -> None:
    """Select a project and mode (build or graph), then run accordingly."""
    import questionary  # noqa: PLC0415

    check_models()

    project_dir = select_project()
    _console.print(
        f"\n[bold]Project:[/] [cyan]{project_dir.name}[/]  [dim]({project_dir})[/]"
    )

    proj_data = load_project_budget(project_dir)
    proj_tokens = proj_data.get("total_tokens", 0)
    proj_calls = proj_data.get("total_calls", len(proj_data["sessions"]))
    if proj_calls > 0:
        _console.print(
            f"  [dim]Accumulated usage: {_format_tokens(proj_tokens)} tokens over {proj_calls} call(s)[/]"
        )

    ensure_workspace_dirs(project_dir)
    _validate_roadmap(project_dir)

    # â”€â”€ Mode selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    mode = questionary.select(
        "What would you like to do?",
        choices=[
            questionary.Choice(
                title="â–¶  Run pipeline          (compact logs â€” errors shown inline)",
                value="build:compact",
            ),
            questionary.Choice(
                title="â–¶  Run pipeline (verbose) (stream full agent output)",
                value="build:verbose",
            ),
            questionary.Choice(
                title="ğŸ” Show dependency graph (terminal)", value="graph"
            ),
            questionary.Choice(
                title="ğŸŒ Show dependency graph (HTML)", value="graph:html"
            ),
            questionary.Choice(
                title="ğŸ“Š Dry run â€” estimate cost & time", value="dryrun"
            ),
            questionary.Choice(
                title="ğŸ“ Generate ROADMAP from description", value="plan"
            ),
            questionary.Choice(title="ğŸŒ Launch Web UI", value="webui"),
            questionary.Choice(
                title=(
                    "ğŸ”„ Regenerate project AGENTS.md"
                    if (project_dir / "AGENTS.md").exists()
                    else "ğŸ“ Generate project AGENTS.md"
                ),
                value="agents",
            ),
        ],
        use_shortcuts=False,
    ).ask()

    if mode is None:
        sys.exit(0)

    if mode == "graph":
        from runner.roadmap import print_dependency_graph  # noqa: PLC0415

        print_dependency_graph(project_dir)
        return

    if mode == "graph:html":
        from runner.graph_html import generate_graph_html  # noqa: PLC0415

        generate_graph_html(project_dir)
        return

    if mode == "dryrun":
        from runner.dryrun import dry_run  # noqa: PLC0415

        dry_run(project_dir)
        return

    if mode == "plan":
        from runner.plan import generate_roadmap_interactive  # noqa: PLC0415

        generate_roadmap_interactive(project_dir.name)
        return

    if mode == "webui":
        try:
            from runner.web.app import start_server  # noqa: PLC0415

            start_server()
        except ImportError as e:
            _console.print(f"[red]{e}[/]")
        return

    if mode == "agents":
        generate_project_agents_md(project_dir)
        return

    # Derive verbose flag from the selected mode.
    verbose = mode == "build:verbose"
    set_verbose(verbose)
    if verbose:
        _console.print("[dim]Log mode: verbose (full agent output)[/]")
    else:
        _console.print(
            "[dim]Log mode: compact (errors shown inline â€” full output in logs/)[/]"
        )

    # â”€â”€ Build mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_tasks = get_tasks(project_dir)
    graph = parse_task_graph(project_dir)
    first_task = all_tasks[0] if all_tasks else None

    # Print completed tasks.
    for task in all_tasks:
        if task_done(task, project_dir):
            _console.print(f"[dim]âœ“ Skipping (done):[/] {task}")

    # Handle resume â€” process the interrupted task sequentially, then continue
    # with graph-based scheduling.
    saved = load_runner_state(project_dir)
    if saved and saved["current_task"]:
        resume_task = saved["current_task"]
        if not task_done(resume_task, project_dir):
            _console.print(
                f"[yellow]â–¶ Resuming[/] '{resume_task}' from attempt {saved['attempt'] + 1}"
            )
            process_task(
                resume_task,
                project_dir,
                resume_attempt=saved["attempt"],
                resume_fix_logs=saved.get("fix_logs"),
            )

    # â”€â”€ Graph-based scheduling loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    while True:
        done_set = {t for t in all_tasks if task_done(t, project_dir)}
        ready = get_ready_tasks(all_tasks, graph, done_set, project_dir)

        if not ready:
            break

        # Milestone tasks are barriers â€” process alone, never in parallel.
        if is_milestone_task(ready[0], project_dir):
            process_milestone(ready[0], project_dir)
            continue

        # The first task (project setup) always runs alone.
        if first_task and first_task in ready:
            process_task(first_task, project_dir)
            continue

        # Filter out any milestone tasks from parallel batches (they wait).
        buildable = [t for t in ready if not is_milestone_task(t, project_dir)]
        if not buildable:
            # Only milestones left but their deps aren't met yet â€” should not
            # happen with a valid graph, but guard against it.
            break

        # Single task or parallelism disabled â†’ sequential.
        if len(buildable) == 1 or MAX_PARALLEL_AGENTS <= 1:
            process_task(buildable[0], project_dir)
            continue

        # Multiple independent tasks â†’ parallel build.
        batch = buildable[:MAX_PARALLEL_AGENTS]
        process_parallel_batch(batch, project_dir)

    # â”€â”€ Pipeline complete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    done_set = {t for t in all_tasks if task_done(t, project_dir)}
    _console.print(
        f"\n[green bold]Pipeline complete:[/] {len(done_set)}/{len(all_tasks)} tasks done."
    )
    try:
        from runner.notify import send_notification  # noqa: PLC0415

        send_notification(
            project_dir,
            "pipeline_done",
            status="ok",
            details={"completed": len(done_set), "total": len(all_tasks)},
        )
    except Exception:  # noqa: BLE001
        pass
